"""
2^5 Factorial Design ì‹¤í—˜ì„ ìœ„í•œ ì¼ê´€ì„± ë¶„ì„ê¸°
BERT Multilingual ê¸°ë°˜ BERTScoreë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ì¼ê´€ì„± ì¸¡ì •
"""

import re
import numpy as np
from bert_score import score
from itertools import combinations
from typing import List, Dict

class ConsistencyAnalyzer:
    def __init__(self, model_type='bert-base-multilingual-cased'):
        """
        ì¼ê´€ì„± ë¶„ì„ê¸° ì´ˆê¸°í™”
        
        Args:
            model_type (str): BERT Multilingual ëª¨ë¸ëª… (í•œêµ­ì–´ ì§€ì› ë‹¤êµ­ì–´ ëª¨ë¸)
        """
        self.model_type = model_type
        print(f"BERT Multilingual ê¸°ë°˜ ì¼ê´€ì„± ë¶„ì„ê¸° ì´ˆê¸°í™”: {model_type}")
        
    def clean_text(self, text):
        """
        í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ê°œí–‰, ê°•ì¡°, ë¦¬ìŠ¤íŠ¸ ê¸°í˜¸ ë“± ì œê±°)
        
        Args:
            text (str): ì›ë³¸ í…ìŠ¤íŠ¸
            
        Returns:
            str: ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸
        """
        text = text.replace('\n', ' ')
        text = re.sub(r'\*\*(.*?)\*\*', r'\1', text)  # **ê°•ì¡°** ì œê±°
        text = re.sub(r'\d+\.\s*', '', text)           # ìˆ«ì ë¦¬ìŠ¤íŠ¸ ì œê±°
        text = re.sub(r'\s+', ' ', text)               # ì¤‘ë³µ ê³µë°± ì œê±°
        return text.strip()
        
    def compute_bert_multilingual_similarity(self, response_list):
        """
        BERT Multilingual ê¸°ë°˜ BERTScoreë¥¼ ì‚¬ìš©í•œ ì‘ë‹µ ê°„ í‰ê·  ìœ ì‚¬ë„ ê³„ì‚°
        
        Args:
            response_list (list): AI ì‘ë‹µë“¤ì˜ ë¦¬ìŠ¤íŠ¸
            
        Returns:
            float: í‰ê·  BERT Multilingual BERTScore F1 (None if error)
        """
        if len(response_list) < 2:
            return None
            
        try:
            # í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
            cleaned_responses = [self.clean_text(resp) for resp in response_list]
            
            # ëª¨ë“  ì‘ë‹µ ìŒì— ëŒ€í•´ BERTScore ê³„ì‚°
            similarities = []
            
            for resp1, resp2 in combinations(cleaned_responses, 2):
                # BERT Multilingualì„ ì‚¬ìš©í•œ BERTScore ê³„ì‚°
                P, R, F1 = score(
                    [resp2], [resp1], 
                    model_type=self.model_type,
                    lang="ko", 
                    verbose=False
                )
                similarities.append(F1.item())
            
            # ëª¨ë“  ìŒì˜ í‰ê·  ìœ ì‚¬ë„ ë°˜í™˜
            return np.mean(similarities)
            
        except Exception as e:
            print(f"BERT Multilingual BERTScore ê³„ì‚° ì˜¤ë¥˜: {e}")
            return None
    
    def analyze_consistency_for_experiment(self, experimental_data: List[Dict]) -> List[Dict]:
        """
        2^5 Factorial Design ì‹¤í—˜ ë°ì´í„°ì˜ ëª¨ë“  ì‘ë‹µì— ëŒ€í•´ BERT Multilingual ê¸°ë°˜ ì¼ê´€ì„± ë¶„ì„
        
        Args:
            experimental_data (list): ì‘ë‹µì´ í¬í•¨ëœ ì‹¤í—˜ ë°ì´í„°
            
        Returns:
            list: ì¼ê´€ì„± ì ìˆ˜ê°€ ì¶”ê°€ëœ ì‹¤í—˜ ë°ì´í„°
        """
        print("BERT Multilingual ê¸°ë°˜ ì¼ê´€ì„± ë¶„ì„ ì‹œì‘...")
        
        for i, data in enumerate(experimental_data):
            print(f"ë¶„ì„ ì¤‘ {i+1}/{len(experimental_data)}: "
                  f"ì¡°ê±´ {data['condition_id']} - {data['category']}")
            
            responses = data.get('responses', [])
            
            if len(responses) >= 2:
                # BERT Multilingual BERTScore ê³„ì‚°
                bert_multilingual_similarity = self.compute_bert_multilingual_similarity(responses)
                data['bert_multilingual_similarity'] = bert_multilingual_similarity
                
                print(f"  â†’ BERT Multilingual ìœ ì‚¬ë„: {bert_multilingual_similarity:.4f}")
            else:
                data['bert_multilingual_similarity'] = None
                print(f"  â†’ ì‘ë‹µ ë¶€ì¡± (ì‘ë‹µ ìˆ˜: {len(responses)})")
        
        print("BERT Multilingual ê¸°ë°˜ ì¼ê´€ì„± ë¶„ì„ ì™„ë£Œ!")
        return experimental_data
    
    def compute_consistency_statistics(self, experimental_data: List[Dict]) -> Dict:
        """
        2^5 Factorial Design ì¡°ê±´ë³„ BERT Multilingual ìœ ì‚¬ë„ í†µê³„ ê³„ì‚°
        
        Args:
            experimental_data (list): ì¼ê´€ì„± ì ìˆ˜ê°€ í¬í•¨ëœ ì‹¤í—˜ ë°ì´í„°
            
        Returns:
            dict: ì¡°ê±´ë³„ í†µê³„ ìš”ì•½
        """
        # ìš”ì¸ë³„ ë°ì´í„° ê·¸ë£¹í™”
        factor_groups = {}
        
        for data in experimental_data:
            if data.get('bert_multilingual_similarity') is not None:
                # ê° ìš”ì¸ë³„ë¡œ ê·¸ë£¹í™”
                for factor_name, factor_value in data['factor_combination'].items():
                    if factor_name not in factor_groups:
                        factor_groups[factor_name] = {}
                    
                    if factor_value not in factor_groups[factor_name]:
                        factor_groups[factor_name][factor_value] = []
                    
                    factor_groups[factor_name][factor_value].append(data['bert_multilingual_similarity'])
                
                # ì¹´í…Œê³ ë¦¬ë³„ë¡œë„ ê·¸ë£¹í™”
                category = data['category']
                if 'category' not in factor_groups:
                    factor_groups['category'] = {}
                
                if category not in factor_groups['category']:
                    factor_groups['category'][category] = []
                
                factor_groups['category'][category].append(data['bert_multilingual_similarity'])
        
        # í†µê³„ ê³„ì‚°
        statistics = {}
        for factor_name, factor_values in factor_groups.items():
            statistics[factor_name] = {}
            
            for factor_value, similarities in factor_values.items():
                if similarities:
                    similarities_array = np.array(similarities)
                    statistics[factor_name][factor_value] = {
                        'n_samples': len(similarities),
                        'mean': similarities_array.mean(),
                        'std': similarities_array.std(),
                        'min': similarities_array.min(),
                        'max': similarities_array.max()
                    }
        
        return statistics
    
    def print_consistency_summary(self, statistics: Dict):
        """
        2^5 Factorial Design BERT Multilingual ê¸°ë°˜ ì¼ê´€ì„± ë¶„ì„ ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        
        Args:
            statistics (dict): ìš”ì¸ë³„ í†µê³„ ë°ì´í„°
        """
        print("\n" + "="*80)
        print("2^5 Factorial Design BERT Multilingual ì¼ê´€ì„± ë¶„ì„ ê²°ê³¼")
        print("="*80)
        
        for factor_name, factor_values in statistics.items():
            print(f"\nğŸ“Š ìš”ì¸: {factor_name}")
            print("-" * 60)
            
            for factor_value, stats in factor_values.items():
                print(f"\n  ğŸ¯ {factor_value}")
                print(f"     ìƒ˜í”Œ ìˆ˜: {stats['n_samples']}")
                print(f"     BERT Multilingual ìœ ì‚¬ë„: {stats['mean']:.4f} (Â±{stats['std']:.4f})")
                print(f"     ë²”ìœ„: {stats['min']:.4f} ~ {stats['max']:.4f}")
        
        # ì „ì²´ ìš”ì•½
        print(f"\nğŸ“ˆ ìš”ì¸ë³„ íš¨ê³¼ í¬ê¸° (ìµœëŒ€ê°’ - ìµœì†Œê°’)")
        print("-" * 60)
        
        for factor_name, factor_values in statistics.items():
            if len(factor_values) >= 2:
                means = [stats['mean'] for stats in factor_values.values()]
                effect_size = max(means) - min(means)
                print(f"  â€¢ {factor_name}: {effect_size:.4f}")

if __name__ == "__main__":
    print("ğŸ§ª 2^5 Factorial Design ì¼ê´€ì„± ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ë°ì´í„°
    sample_data = [
        {
            'condition_id': 1,
            'category': 'ì¸ì„±',
            'factor_combination': {
                'prompt_language': 'korean',
                'model': 'gpt-4o-mini',
                'role_assignment': 'with_role',
                'context_provision': 'no_context',
                'explicitness': 'low'
            },
            'responses': [
                'ì¹œêµ¬ë¥¼ ë•ëŠ” ê²ƒì€ ì¸ê°„ì˜ ê¸°ë³¸ì ì¸ ë„ë•ì  ì˜ë¬´ì…ë‹ˆë‹¤.',
                'ì–´ë ¤ìš´ ìƒí™©ì— ìˆëŠ” ì¹œêµ¬ë¥¼ ë„ì™€ì£¼ëŠ” ê²ƒì€ ìš°ì •ì˜ í•µì‹¬ì´ë©°, ìƒí˜¸ë¶€ì¡°ì˜ ì •ì‹ ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.',
                'íƒ€ì¸ì„ ë•ëŠ” í–‰ìœ„ëŠ” ê³µê°ëŠ¥ë ¥ì„ ë°œíœ˜í•˜ëŠ” ê²ƒì´ë©°, ì‚¬íšŒì  ìœ ëŒ€ê°ì„ ê°•í™”í•©ë‹ˆë‹¤.'
            ]
        }
    ]
    
    analyzer = ConsistencyAnalyzer()
    
    # ì¼ê´€ì„± ë¶„ì„
    analyzed_data = analyzer.analyze_consistency_for_experiment(sample_data)
    
    # í†µê³„ ê³„ì‚°
    stats = analyzer.compute_consistency_statistics(analyzed_data)
    
    # ê²°ê³¼ ì¶œë ¥
    analyzer.print_consistency_summary(stats)
    
    print("\nâœ… ì¼ê´€ì„± ë¶„ì„ê¸° í…ŒìŠ¤íŠ¸ ì™„ë£Œ!") 