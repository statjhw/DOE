"""
2^5 Factorial Designìš© AI ì‘ë‹µ ìˆ˜ì§‘ê¸°
ë‹¤ì–‘í•œ ëª¨ë¸ê³¼ í”„ë¡¬í”„íŠ¸ ì¡°í•©ìœ¼ë¡œë¶€í„° ì‘ë‹µ ìˆ˜ì§‘
"""

import openai
import time
import random
from typing import Dict, List

class FactorialResponseCollector:
    def __init__(self, api_key: str, api_delay: float = 1.0):
        """
        ì‘ë‹µ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
        
        Args:
            api_key (str): OpenAI API í‚¤
            api_delay (float): API í˜¸ì¶œ ê°„ ëŒ€ê¸°ì‹œê°„ (ì´ˆ)
        """
        self.client = openai.OpenAI(api_key=api_key)
        self.api_delay = api_delay
        
    def collect_single_response(self, prompt: str, model: str, temperature: float = 0.7, max_tokens: int = 500) -> str:
        """
        ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ ì‘ë‹µ ìˆ˜ì§‘
        
        Args:
            prompt (str): ì…ë ¥ í”„ë¡¬í”„íŠ¸
            model (str): ì‚¬ìš©í•  ëª¨ë¸
            temperature (float): ì‘ë‹µ ë‹¤ì–‘ì„±
            max_tokens (int): ìµœëŒ€ í† í° ìˆ˜
            
        Returns:
            str: AI ì‘ë‹µ
        """
        try:
            response = self.client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            print(f"   âš ï¸ API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
            return f"[ERROR: {e}]"
    
    def collect_multiple_responses(self, prompt: str, model: str, n_responses: int = 3, temperature: float = 0.7) -> List[str]:
        """
        ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ì— ëŒ€í•´ ì—¬ëŸ¬ ì‘ë‹µ ìˆ˜ì§‘
        
        Args:
            prompt (str): ì…ë ¥ í”„ë¡¬í”„íŠ¸
            model (str): ì‚¬ìš©í•  ëª¨ë¸
            n_responses (int): ìˆ˜ì§‘í•  ì‘ë‹µ ìˆ˜
            temperature (float): ì‘ë‹µ ë‹¤ì–‘ì„±
            
        Returns:
            List[str]: ìˆ˜ì§‘ëœ ì‘ë‹µë“¤
        """
        responses = []
        
        for i in range(n_responses):
            response = self.collect_single_response(prompt, model, temperature)
            responses.append(response)
            
            # API í˜¸ì¶œ ê°„ ëŒ€ê¸° (ë§ˆì§€ë§‰ í˜¸ì¶œ ì œì™¸)
            if i < n_responses - 1:
                time.sleep(self.api_delay)
        
        print(f"  â†’ {len(responses)}ê°œ ì‘ë‹µ ìˆ˜ì§‘ ì™„ë£Œ")
        return responses
    
    def collect_responses_for_condition(self, condition: Dict, n_responses: int = 3, temperature: float = 0.7) -> Dict:
        """
        íŠ¹ì • ì¡°ê±´ì— ëŒ€í•œ AI ì‘ë‹µ ìˆ˜ì§‘
        
        Args:
            condition (Dict): ì‹¤í—˜ ì¡°ê±´ (ìƒì„±ëœ í”„ë¡¬í”„íŠ¸ í¬í•¨)
            n_responses (int): ìˆ˜ì§‘í•  ì‘ë‹µ ìˆ˜
            temperature (float): ì‘ë‹µ ìƒì„± ì˜¨ë„
            
        Returns:
            Dict: ì‘ë‹µì´ í¬í•¨ëœ ì¡°ê±´ ë°ì´í„°
        """
        # í”„ë¡¬í”„íŠ¸ì™€ ëª¨ë¸ ì •ë³´ ì¶”ì¶œ
        prompt = condition['generated_prompt']
        model = condition['factor_combination']['model']
        
        print(f"  ì¡°ê±´ ID: {condition['condition_id']}")
        print(f"  ì¹´í…Œê³ ë¦¬: {condition['category']}")
        print(f"  ìš”ì¸ ì¡°í•©: {', '.join([f'{k}:{v}' for k, v in condition['factor_combination'].items()])}")
        
        # ì‘ë‹µ ìˆ˜ì§‘
        responses = self.collect_multiple_responses(
            prompt=prompt,
            model=model, 
            n_responses=n_responses,
            temperature=temperature
        )
        
        # ê²°ê³¼ ì €ì¥
        condition.update({
            'responses': responses,
            'n_responses': len(responses),
            'model_used': model,
            'temperature_used': temperature
        })
        
        return condition
    
    def collect_responses_for_experiment(self, experimental_data: List[Dict], n_responses: int = 3, temperature: float = 0.7) -> List[Dict]:
        """
        ì „ì²´ ì‹¤í—˜ì— ëŒ€í•´ ì‘ë‹µ ìˆ˜ì§‘
        
        Args:
            experimental_data (List[Dict]): í”„ë¡¬í”„íŠ¸ê°€ í¬í•¨ëœ ì‹¤í—˜ ë°ì´í„°
            n_responses (int): ê° ì¡°ê±´ë‹¹ ìˆ˜ì§‘í•  ì‘ë‹µ ìˆ˜
            temperature (float): ì‘ë‹µ ë‹¤ì–‘ì„±
            
        Returns:
            List[Dict]: ì‘ë‹µì´ ì¶”ê°€ëœ ì‹¤í—˜ ë°ì´í„°
        """
        print(f"\nğŸ’¬ AI ì‘ë‹µ ìˆ˜ì§‘ ì‹œì‘ (ê° ì¡°ê±´ë‹¹ {n_responses}ê°œ)")
        print("=" * 60)
        
        total_api_calls = len(experimental_data) * n_responses
        print(f"ì˜ˆìƒ API í˜¸ì¶œ ìˆ˜: {total_api_calls}")
        print(f"ì˜ˆìƒ ì†Œìš”ì‹œê°„: {total_api_calls * self.api_delay / 60:.1f}ë¶„")
        
        for i, condition in enumerate(experimental_data):
            print(f"\nì¡°ê±´ {i+1}/{len(experimental_data)} ì²˜ë¦¬ ì¤‘...")
            
            # ì‘ë‹µ ìˆ˜ì§‘
            condition = self.collect_responses_for_condition(condition, n_responses, temperature)
        
        print(f"\nâœ… ì‘ë‹µ ìˆ˜ì§‘ ì™„ë£Œ!")
        print(f"   ì´ ì¡°ê±´ ìˆ˜: {len(experimental_data)}")
        print(f"   ì´ ì‘ë‹µ ìˆ˜: {len(experimental_data) * n_responses}")
        
        return experimental_data
    
    def collect_by_model_batch(self, experimental_data: List[Dict], n_responses: int = 3, temperature: float = 0.7) -> List[Dict]:
        """
        ëª¨ë¸ë³„ë¡œ ë¬¶ì–´ì„œ ì‘ë‹µ ìˆ˜ì§‘ (íš¨ìœ¨ì„± í–¥ìƒ)
        
        Args:
            experimental_data (List[Dict]): ì‹¤í—˜ ë°ì´í„°
            n_responses (int): ê° ì¡°ê±´ë‹¹ ì‘ë‹µ ìˆ˜
            temperature (float): ì‘ë‹µ ë‹¤ì–‘ì„±
            
        Returns:
            List[Dict]: ì‘ë‹µì´ ì¶”ê°€ëœ ì‹¤í—˜ ë°ì´í„°
        """
        print(f"\nğŸ’¬ ëª¨ë¸ë³„ ë°°ì¹˜ ì‘ë‹µ ìˆ˜ì§‘ ì‹œì‘")
        print("=" * 60)
        
        # ëª¨ë¸ë³„ë¡œ ê·¸ë£¹í™”
        model_groups = {}
        for condition in experimental_data:
            model = condition['factor_combination']['model']  # ìˆ˜ì •: ì˜¬ë°”ë¥¸ ê²½ë¡œ
            if model not in model_groups:
                model_groups[model] = []
            model_groups[model].append(condition)
        
        print(f"ëª¨ë¸ë³„ ì¡°ê±´ ìˆ˜:")
        for model, conditions in model_groups.items():
            print(f"  â€¢ {model}: {len(conditions)}ê°œ ì¡°ê±´")
        
        # ëª¨ë¸ë³„ë¡œ ìˆœì°¨ ì²˜ë¦¬
        all_processed = []
        
        for model_name, conditions in model_groups.items():
            print(f"\nğŸ¤– {model_name} ëª¨ë¸ ì‘ë‹µ ìˆ˜ì§‘ ì¤‘...")
            print("-" * 40)
            
            for i, condition in enumerate(conditions):
                print(f"  {i+1}/{len(conditions)}: ì¡°ê±´ {condition['condition_id']}")
                condition = self.collect_responses_for_condition(condition, n_responses, temperature)
                all_processed.append(condition)
        
        # ì›ë˜ ìˆœì„œë¡œ ì •ë ¬
        all_processed.sort(key=lambda x: x['condition_id'])
        
        print(f"\nâœ… ëª¨ë¸ë³„ ë°°ì¹˜ ìˆ˜ì§‘ ì™„ë£Œ!")
        return all_processed
    
    def _format_factors(self, factor_combination: Dict) -> str:
        """ìš”ì¸ ì¡°í•©ì„ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ í¬ë§·"""
        formatted = []
        for factor, value in factor_combination.items():
            formatted.append(f"{factor}:{value}")
        return ", ".join(formatted)
    
    def estimate_cost(self, experimental_data: List[Dict], n_responses: int = 3) -> Dict:
        """
        ì‹¤í—˜ ë¹„ìš© ì¶”ì •
        
        Args:
            experimental_data (List[Dict]): ì‹¤í—˜ ë°ì´í„°
            n_responses (int): ê° ì¡°ê±´ë‹¹ ì‘ë‹µ ìˆ˜
            
        Returns:
            Dict: ë¹„ìš© ì¶”ì • ì •ë³´
        """
        # ëª¨ë¸ë³„ ë¹„ìš© (2024ë…„ ê¸°ì¤€)
        model_costs = {
            'gpt-3.5-turbo': {'input': 0.0015, 'output': 0.002},  # per 1K tokens
            'gpt-4o-mini': {'input': 0.00015, 'output': 0.0006}   # per 1K tokens
        }
        
        model_counts = {}
        for condition in experimental_data:
            model = condition['factor_combination']['model']  # ìˆ˜ì •: ì˜¬ë°”ë¥¸ ê²½ë¡œ
            model_counts[model] = model_counts.get(model, 0) + 1
        
        total_cost = 0
        cost_breakdown = {}
        
        for model, count in model_counts.items():
            if model in model_costs:
                # ëŒ€ëµì ì¸ í† í° ìˆ˜ ì¶”ì • (í”„ë¡¬í”„íŠ¸ 150 í† í°, ì‘ë‹µ 200 í† í°)
                input_tokens = count * n_responses * 150
                output_tokens = count * n_responses * 200
                
                model_cost = (
                    (input_tokens / 1000) * model_costs[model]['input'] +
                    (output_tokens / 1000) * model_costs[model]['output']
                )
                
                cost_breakdown[model] = {
                    'conditions': count,
                    'total_responses': count * n_responses,
                    'estimated_cost': model_cost
                }
                total_cost += model_cost
        
        return {
            'total_estimated_cost': total_cost,
            'model_breakdown': cost_breakdown,
            'total_api_calls': len(experimental_data) * n_responses
        }

# ì‹¤í–‰ ì˜ˆì‹œ
if __name__ == "__main__":
    import os
    from dotenv import load_dotenv
    
    load_dotenv()
    api_key = os.getenv('OPENAI_API_KEY')
    
    print("ğŸ§ª 2^5 Factorial Response Collector í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    # í…ŒìŠ¤íŠ¸ìš© ìƒ˜í”Œ ì¡°ê±´
    test_condition = {
        'condition_id': 1,
        'category': 'ì¸ì„±',
        'base_question': 'ì–´ë ¤ì›€ì— ì²˜í•œ ì¹œêµ¬ë¥¼ ë„ì™€ì•¼ í•˜ëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€?',
        'generated_prompt': 'ì–´ë ¤ì›€ì— ì²˜í•œ ì¹œêµ¬ë¥¼ ë„ì™€ì•¼ í•˜ëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€? í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.',
        'factor_combination': {
            'prompt_language': 'korean',
            'model': 'gpt-4o-mini',
            'role_assignment': 'no_role',
            'context_provision': 'no_context',
            'explicitness': 'low'
        }
    }
    
    # ì‘ë‹µ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”
    collector = FactorialResponseCollector(api_key, api_delay=0.5)
    
    # ë¹„ìš© ì¶”ì •
    cost_info = collector.estimate_cost([test_condition], n_responses=2)
    print(f"\nğŸ’° ë¹„ìš© ì¶”ì •:")
    print(f"   ì´ ì˜ˆìƒ ë¹„ìš©: ${cost_info['total_estimated_cost']:.3f}")
    print(f"   ì´ API í˜¸ì¶œ: {cost_info['total_api_calls']}")
    
    print("\nâœ… Response Collector ì´ˆê¸°í™” ì™„ë£Œ!") 